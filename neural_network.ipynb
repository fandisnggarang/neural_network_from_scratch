{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class Neural_Network(): \n",
    "    def __init__(self, input_size, hidden_size, output_size, l_rate, activation='sigmoid', optimizer ='sgd'):\n",
    "        # initialize data_size, l_rate, and optimizer\n",
    "        self.input_size   = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.output_size  = output_size\n",
    "        self.l_rate       = l_rate\n",
    "        self.optimizer    = optimizer\n",
    "\n",
    "        # all results in forward-backward are stored here! \n",
    "        self.storage = {}\n",
    "        self.grads   = {}\n",
    "        \n",
    "        # initialize momentum_optimizer\n",
    "        if self.optimizer == 'momentum': \n",
    "            self.momentum_opt = self.initialize_momentum_optimizer()\n",
    "        \n",
    "        # initialize activation function\n",
    "        if activation == 'sigmoid': \n",
    "            self.activation = self.sigmoid\n",
    "        elif activation == 'relu': \n",
    "            self.activation = self.relu\n",
    "        else: \n",
    "            raise ValueError(\"Please choose 'sigmoid' or 'relu' for activation function\")\n",
    "        \n",
    "        # intialize weight and bias parameter\n",
    "        self.params = self.initialize()\n",
    "        \n",
    "    def apply_dropout(self, x, dropout_prob=0.05):\n",
    "        # dropout is only used if it is activated and run in training \n",
    "        epsilon = 1e-8  # prevent division by zero or small denominators\n",
    "\n",
    "        self.dropout_mask = np.random.binomial(1, 1-dropout_prob, size = x.shape)\n",
    "\n",
    "        # make sure that dropout_mask's data type is the same with x's\n",
    "        self.dropout_mask = self.dropout_mask.astype(x.dtype)\n",
    "        \n",
    "        # add epsilon, just in case 1-self.dropout_prob < epsilon, we go with epsilon\n",
    "        # this is added to avoid denominator being to small.\n",
    "        x = (x * self.dropout_mask) / max((1 - dropout_prob, epsilon))\n",
    "        return x\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        sig = 1 / (1 + np.exp(-x))\n",
    "        if derivative:\n",
    "            return sig * (1 - sig)\n",
    "        return sig\n",
    "    \n",
    "    def relu(self, x, derivative=False): \n",
    "        if derivative:\n",
    "            x = np.where(x < 0, 0, x)\n",
    "            x = np.where(x >= 0, 1, x)\n",
    "            return x \n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def softmax(self, x): \n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis = 0) \n",
    "\n",
    "    def initialize(self):\n",
    "        # if sigmoid, we use Xavier (1.0), else HE (2.0)\n",
    "        factor = 1.0 if self.activation == self.sigmoid else 2.0\n",
    "        \n",
    "        scaling_1 = np.sqrt(factor/self.input_size)\n",
    "        scaling_2 = np.sqrt(factor/self.hidden_size[0])\n",
    "        scaling_3 = np.sqrt(factor/self.hidden_size[1])\n",
    "        scaling_4 = np.sqrt(factor/self.hidden_size[2])\n",
    "\n",
    "        # create value of each parameter randomly and scale the values using the choosen factor\n",
    "        params = {\n",
    "        'w1':np.random.randn(self.hidden_size[0], self.input_size) * scaling_1,\n",
    "        'b1':np.zeros((self.hidden_size[0], 1)) * scaling_1,\n",
    "\n",
    "        'w2':np.random.randn(self.hidden_size[1], self.hidden_size[0]) * scaling_2,\n",
    "        'b2':np.zeros((self.hidden_size[1], 1)) * scaling_2,\n",
    "        \n",
    "        'w3':np.random.randn(self.hidden_size[2], self.hidden_size[1]) * scaling_3,\n",
    "        'b3':np.zeros((self.hidden_size[2], 1)) * scaling_3, \n",
    "\n",
    "        'w4':np.random.randn(self.output_size, self.hidden_size[2]) * scaling_4,\n",
    "        'b4':np.zeros((self.output_size, 1)) * scaling_4   \n",
    "        }\n",
    "        \n",
    "        return params        \n",
    "\n",
    "    def forward(self, x, training=True, dropout=True):\n",
    "        self.storage['x'] = x\n",
    "         \n",
    "        # The multiplication happens first, and then the neuron is deactivated by dropout\n",
    "        # input layer -> hidden layer 1\n",
    "        self.storage['z1'] = np.matmul(self.params['w1'], self.storage['x'].T) + self.params['b1']\n",
    "\n",
    "        self.storage['a1'] = self.activation(self.storage['z1'], derivative=False)\n",
    "\n",
    "        # drop some neuron in hidden layer 1\n",
    "        if training and dropout:\n",
    "            self.storage['a1'] = self.apply_dropout(self.storage['a1']) \n",
    "\n",
    "        # hidden layer 1 -> hidden layer 2\n",
    "        self.storage['z2'] = np.matmul(self.params['w2'], self.storage['a1']) + self.params['b2']\n",
    "        self.storage['a2'] = self.activation(self.storage['z2'], derivative=False)\n",
    "\n",
    "        # drop some neuron in hidden layer 2\n",
    "        if training and dropout:\n",
    "            self.storage['a2'] = self.apply_dropout(self.storage['a2']) \n",
    "\n",
    "        # hidden layer 2 -> hidden layer 3\n",
    "        self.storage['z3'] = np.matmul(self.params['w3'], self.storage['a2']) + self.params['b3']\n",
    "        self.storage['a3'] = self.activation(self.storage['z3'], derivative=False)\n",
    "\n",
    "        # drop some neuron in hidden layer 3\n",
    "        if training and dropout:\n",
    "            self.storage['a3'] = self.apply_dropout(self.storage['a3']) \n",
    "\n",
    "        # hidden layer 3 -> output layer\n",
    "        self.storage['z4']     = np.matmul(self.params['w4'], self.storage['a3']) + self.params['b4']\n",
    "        self.storage['output'] = self.softmax(self.storage['z4'])\n",
    "\n",
    "        return self.storage['output']\n",
    "        \n",
    "    def cross_entropy_loss(self, output, y, lasso=False, ridge=False, lambda_l=1e-5, lambda_r=1e-5):\n",
    "        # add clipping to avoid log 0 problems\n",
    "        epsilon    = 1e-15\n",
    "        output     = np.clip(output, epsilon, 1.0)\n",
    "        \n",
    "        # computing cross-entropy loss\n",
    "        l_sum = np.sum(np.multiply(y.T, np.log(output)))\n",
    "        m     = y.shape[0]\n",
    "        avg_loss = -(1./m) * l_sum \n",
    "        \n",
    "        # if lasso is implemented\n",
    "        if lasso:\n",
    "            l_reg  = sum(np.abs(values).sum() for key, values in self.params.items() if 'w' in key)\n",
    "            avg_loss += lambda_l * l_reg\n",
    "        \n",
    "        # if ridge is implemented\n",
    "        if ridge:\n",
    "            r_reg  = sum(np.square(values).sum() for key, values in self.params.items() if 'w' in key)\n",
    "            avg_loss += lambda_r * r_reg\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    def accuracy(self, y, output):\n",
    "        return np.mean(np.argmax(y, axis = 1) == np.argmax(output.T, axis = 1))\n",
    "\n",
    "    def backward(self, output, y, lasso=False, ridge=False, lambda_l=1e-5, lambda_r=1e-5): \n",
    "        current_batch_size = y.shape[0]\n",
    "        output = self.storage['output'].copy()\n",
    "\n",
    "        # going back: output layer -> hidden layer 3\n",
    "        dz4 = output - y.T\n",
    "        self.grads['w4'] = (1./current_batch_size) * (dz4 @ self.storage['a3'].T)\n",
    "        self.grads['b4'] = (1./current_batch_size) * np.sum(dz4, axis=1, keepdims=True)\n",
    "\n",
    "        # going back: hidden layer 3 -> hidden layer 2\n",
    "        da3 = self.params['w4'].T @ dz4 \n",
    "        dz3 = da3 * self.sigmoid(self.storage['z3'], derivative = True)\n",
    "        self.grads['w3'] = (1./current_batch_size) * (dz3 @ self.storage['a2'].T)\n",
    "        self.grads['b3'] = (1./current_batch_size) * np.sum(dz3, axis=1, keepdims=True)\n",
    "\n",
    "        # going back: hidden layer 2 -> hidden layer 1\n",
    "        da2 = self.params['w3'].T @ dz3 \n",
    "        dz2 = da2 * self.sigmoid(self.storage['z2'], derivative = True)\n",
    "        self.grads['w2'] = (1./current_batch_size) * (dz2 @ self.storage['a1'].T) \n",
    "        self.grads['b2'] = (1./current_batch_size) * np.sum(dz2, axis=1, keepdims=True)\n",
    "\n",
    "        # going back: hidden layer 1 -> input layer\n",
    "        da1 = self.params['w2'].T @ dz2 \n",
    "        dz1 = da1 * self.sigmoid(self.storage['z1'], derivative = True)\n",
    "        self.grads['w1'] = (1./current_batch_size) * (dz1 @ self.storage['x'])\n",
    "        self.grads['b1'] = (1./current_batch_size) * np.sum(dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        # add lasso and ridge penalty to the weights\n",
    "        for key in [k for k in self.grads if 'w' in k]: \n",
    "            if lasso: \n",
    "                self.grads[key] += lambda_l * np.sign(self.params[key])\n",
    "            if ridge: \n",
    "                self.grads[key] += 2 * lambda_r * self.params[key]\n",
    "                    \n",
    "        return self.grads \n",
    "                        \n",
    "    def apply_gradient_clipping(self, clip_value):\n",
    "        # apply clipping to avoid gradient exploding\n",
    "        for key in [k for k in self.grads if 'w' in k]: \n",
    "            grad_norm = np.linalg.norm(self.grads[key])\n",
    "            if grad_norm > clip_value: \n",
    "                    self.grads[key] *= (clip_value/grad_norm)\n",
    "        return self.grads\n",
    "\n",
    "    def initialize_momentum_optimizer(self):\n",
    "        # initialize momentum \n",
    "        momentum_opt = {\n",
    "            'w1': np.zeros_like(self.params['w1']),\n",
    "            'b1': np.zeros_like(self.params['b1']),\n",
    "            'w2': np.zeros_like(self.params['w2']),\n",
    "            'b2': np.zeros_like(self.params['b2']), \n",
    "            'w3': np.zeros_like(self.params['w3']),\n",
    "            'b3': np.zeros_like(self.params['b3']),\n",
    "            'w4': np.zeros_like(self.params['w4']),\n",
    "            'b4': np.zeros_like(self.params['b4']), \n",
    "        }\n",
    "        return momentum_opt \n",
    "\n",
    "    def optimize(self, beta_momentum=0.9):\n",
    "        # if sgd technique is used\n",
    "        if self.optimizer == 'sgd':\n",
    "            for key in self.params:\n",
    "                self.params[key] -= self.l_rate * self.grads[key]\n",
    "\n",
    "        elif self.optimizer == 'momentum':\n",
    "        # if momentum technique is used\n",
    "            for key in self.params:\n",
    "                self.momentum_opt[key] = (beta_momentum  * self.momentum_opt[key] + (1. - beta_momentum) * self.grads[key])\n",
    "                self.params[key]      -= self.l_rate * self.momentum_opt[key]\n",
    "        else:\n",
    "            raise ValueError(\"We only have 'sgd' and 'momentum'. Please choose one!\")\n",
    "        \n",
    "        return self.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, x_train, y_train, x_test, y_test, epochs, batch_size):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "\n",
    "        # put data train in batch 32 (for example)\n",
    "        for batch_start in range(0, x_train.shape[0], batch_size):\n",
    "            indices = permutation[batch_start:min(batch_start+batch_size, x_train.shape[0])]\n",
    "            batch_x, batch_y = x_train[indices], y_train[indices]\n",
    "\n",
    "            # forward\n",
    "            frw_output = model.forward(batch_x)\n",
    "\n",
    "            # backward\n",
    "            _ = model.backward(frw_output, batch_y)\n",
    "\n",
    "            # uncomment it below if clipping is used\n",
    "            # model.apply_gradient_clipping(clip_value=0.005)\n",
    "\n",
    "            # update\n",
    "            model.optimize()\n",
    "\n",
    "        # Evaluate performance\n",
    "        # train\n",
    "        train_out = model.forward(x_train, training=False, dropout=False)\n",
    "        train_acc = model.accuracy(y_train, train_out)\n",
    "        train_loss= model.cross_entropy_loss(train_out, y_train)\n",
    "\n",
    "        # test\n",
    "        test_out  = model.forward(x_test, training=False, dropout=False)\n",
    "        test_acc  = model.accuracy(y_test, test_out)\n",
    "        test_loss = model.cross_entropy_loss(test_out, y_test)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1} - train acc: {train_acc:.4f}, train loss: {train_loss:.4f}, test acc: {test_acc:.4f}, test loss: {test_loss:.4f}, time: {epoch_time:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create and split data\n",
    "n_samples  = 10000 \n",
    "n_features = 10\n",
    "n_informative = 3\n",
    "n_classes  = 3\n",
    "n_cluster_per_class = 2\n",
    "random_state = 25\n",
    "\n",
    "x, y  = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, \n",
    "                            n_classes=n_classes, n_clusters_per_class=n_cluster_per_class, random_state = random_state)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# preprocess the data\n",
    "x_train_scaled = preprocessing.normalize(x_train)\n",
    "x_test_scaled  = preprocessing.normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# apply one hot encoding to y data\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test  = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model\n",
    "model = Neural_Network(input_size=n_features, hidden_size=[128, 64, 32], output_size=n_classes, l_rate=0.1, activation='sigmoid', optimizer='sgd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train acc: 0.3351, train loss: 1.0975, test acc: 0.3350, test loss: 1.0975, time: 0.49s\n",
      "Epoch 2 - train acc: 0.3325, train loss: 1.1053, test acc: 0.3325, test loss: 1.1051, time: 1.04s\n",
      "Epoch 3 - train acc: 0.4238, train loss: 1.1003, test acc: 0.4275, test loss: 1.1000, time: 1.61s\n",
      "Epoch 4 - train acc: 0.4284, train loss: 1.0896, test acc: 0.4425, test loss: 1.0889, time: 2.75s\n",
      "Epoch 5 - train acc: 0.4155, train loss: 1.0731, test acc: 0.4240, test loss: 1.0716, time: 3.75s\n",
      "Epoch 6 - train acc: 0.4888, train loss: 1.0549, test acc: 0.4985, test loss: 1.0518, time: 4.67s\n",
      "Epoch 7 - train acc: 0.4716, train loss: 1.0470, test acc: 0.4755, test loss: 1.0421, time: 5.48s\n",
      "Epoch 8 - train acc: 0.4527, train loss: 1.0290, test acc: 0.4565, test loss: 1.0211, time: 6.34s\n",
      "Epoch 9 - train acc: 0.4382, train loss: 1.0336, test acc: 0.4510, test loss: 1.0239, time: 6.88s\n",
      "Epoch 10 - train acc: 0.4685, train loss: 1.0165, test acc: 0.4790, test loss: 1.0075, time: 7.40s\n",
      "Epoch 11 - train acc: 0.4439, train loss: 1.0142, test acc: 0.4560, test loss: 1.0045, time: 7.87s\n",
      "Epoch 12 - train acc: 0.4794, train loss: 1.0133, test acc: 0.4915, test loss: 1.0034, time: 8.42s\n",
      "Epoch 13 - train acc: 0.3782, train loss: 1.0249, test acc: 0.3880, test loss: 1.0152, time: 9.00s\n",
      "Epoch 14 - train acc: 0.4691, train loss: 1.0106, test acc: 0.4850, test loss: 1.0004, time: 9.44s\n",
      "Epoch 15 - train acc: 0.4535, train loss: 1.0099, test acc: 0.4675, test loss: 0.9996, time: 9.91s\n",
      "Epoch 16 - train acc: 0.4664, train loss: 1.0087, test acc: 0.4835, test loss: 0.9984, time: 10.32s\n",
      "Epoch 17 - train acc: 0.4823, train loss: 1.0078, test acc: 0.4970, test loss: 0.9971, time: 10.75s\n",
      "Epoch 18 - train acc: 0.4695, train loss: 1.0063, test acc: 0.4830, test loss: 0.9960, time: 11.22s\n",
      "Epoch 19 - train acc: 0.4849, train loss: 1.0065, test acc: 0.4940, test loss: 0.9961, time: 11.66s\n",
      "Epoch 20 - train acc: 0.4636, train loss: 1.0045, test acc: 0.4810, test loss: 0.9945, time: 12.07s\n",
      "Epoch 21 - train acc: 0.4910, train loss: 1.0061, test acc: 0.5020, test loss: 0.9959, time: 12.84s\n",
      "Epoch 22 - train acc: 0.4909, train loss: 1.0006, test acc: 0.5030, test loss: 0.9902, time: 13.52s\n",
      "Epoch 23 - train acc: 0.4920, train loss: 0.9995, test acc: 0.5025, test loss: 0.9899, time: 14.40s\n",
      "Epoch 24 - train acc: 0.4531, train loss: 1.0120, test acc: 0.4565, test loss: 1.0009, time: 14.90s\n",
      "Epoch 25 - train acc: 0.4945, train loss: 0.9908, test acc: 0.5045, test loss: 0.9805, time: 15.44s\n",
      "Epoch 26 - train acc: 0.4905, train loss: 0.9868, test acc: 0.5025, test loss: 0.9777, time: 16.24s\n",
      "Epoch 27 - train acc: 0.4960, train loss: 0.9828, test acc: 0.5120, test loss: 0.9742, time: 16.98s\n",
      "Epoch 28 - train acc: 0.4945, train loss: 0.9763, test acc: 0.5100, test loss: 0.9684, time: 17.45s\n",
      "Epoch 29 - train acc: 0.4918, train loss: 0.9674, test acc: 0.5030, test loss: 0.9598, time: 17.91s\n",
      "Epoch 30 - train acc: 0.5050, train loss: 0.9694, test acc: 0.5190, test loss: 0.9632, time: 18.30s\n",
      "Epoch 31 - train acc: 0.5058, train loss: 0.9528, test acc: 0.5010, test loss: 0.9477, time: 18.69s\n",
      "Epoch 32 - train acc: 0.5224, train loss: 0.9440, test acc: 0.5135, test loss: 0.9393, time: 19.05s\n",
      "Epoch 33 - train acc: 0.4738, train loss: 0.9371, test acc: 0.4690, test loss: 0.9331, time: 19.43s\n",
      "Epoch 34 - train acc: 0.5475, train loss: 0.9307, test acc: 0.5430, test loss: 0.9272, time: 19.94s\n",
      "Epoch 35 - train acc: 0.5314, train loss: 0.9297, test acc: 0.5360, test loss: 0.9257, time: 20.44s\n",
      "Epoch 36 - train acc: 0.5586, train loss: 0.9165, test acc: 0.5545, test loss: 0.9151, time: 21.23s\n",
      "Epoch 37 - train acc: 0.5676, train loss: 0.9117, test acc: 0.5525, test loss: 0.9114, time: 21.69s\n",
      "Epoch 38 - train acc: 0.5744, train loss: 0.9029, test acc: 0.5630, test loss: 0.9020, time: 22.11s\n",
      "Epoch 39 - train acc: 0.5690, train loss: 0.8999, test acc: 0.5605, test loss: 0.8999, time: 22.48s\n",
      "Epoch 40 - train acc: 0.5624, train loss: 0.8954, test acc: 0.5610, test loss: 0.8962, time: 22.88s\n",
      "Epoch 41 - train acc: 0.5799, train loss: 0.8907, test acc: 0.5625, test loss: 0.8925, time: 23.24s\n",
      "Epoch 42 - train acc: 0.5771, train loss: 0.8914, test acc: 0.5700, test loss: 0.8940, time: 23.63s\n",
      "Epoch 43 - train acc: 0.5804, train loss: 0.8846, test acc: 0.5730, test loss: 0.8872, time: 24.04s\n",
      "Epoch 44 - train acc: 0.5851, train loss: 0.8833, test acc: 0.5715, test loss: 0.8869, time: 24.45s\n",
      "Epoch 45 - train acc: 0.5835, train loss: 0.8818, test acc: 0.5710, test loss: 0.8846, time: 24.87s\n",
      "Epoch 46 - train acc: 0.5889, train loss: 0.8789, test acc: 0.5715, test loss: 0.8821, time: 25.30s\n",
      "Epoch 47 - train acc: 0.5691, train loss: 0.8861, test acc: 0.5600, test loss: 0.8900, time: 25.71s\n",
      "Epoch 48 - train acc: 0.5891, train loss: 0.8748, test acc: 0.5735, test loss: 0.8784, time: 26.11s\n",
      "Epoch 49 - train acc: 0.5740, train loss: 0.8762, test acc: 0.5645, test loss: 0.8814, time: 26.53s\n",
      "Epoch 50 - train acc: 0.5869, train loss: 0.8730, test acc: 0.5745, test loss: 0.8774, time: 26.91s\n",
      "Epoch 51 - train acc: 0.5889, train loss: 0.8743, test acc: 0.5725, test loss: 0.8786, time: 27.28s\n",
      "Epoch 52 - train acc: 0.5880, train loss: 0.8676, test acc: 0.5765, test loss: 0.8728, time: 27.70s\n",
      "Epoch 53 - train acc: 0.5859, train loss: 0.8687, test acc: 0.5765, test loss: 0.8744, time: 28.45s\n",
      "Epoch 54 - train acc: 0.5756, train loss: 0.8738, test acc: 0.5680, test loss: 0.8778, time: 29.31s\n",
      "Epoch 55 - train acc: 0.5873, train loss: 0.8672, test acc: 0.5770, test loss: 0.8719, time: 30.33s\n",
      "Epoch 56 - train acc: 0.5883, train loss: 0.8629, test acc: 0.5755, test loss: 0.8690, time: 31.14s\n",
      "Epoch 57 - train acc: 0.5887, train loss: 0.8650, test acc: 0.5790, test loss: 0.8698, time: 31.68s\n",
      "Epoch 58 - train acc: 0.5945, train loss: 0.8613, test acc: 0.5770, test loss: 0.8674, time: 32.14s\n",
      "Epoch 59 - train acc: 0.5894, train loss: 0.8678, test acc: 0.5770, test loss: 0.8721, time: 32.70s\n",
      "Epoch 60 - train acc: 0.5981, train loss: 0.8558, test acc: 0.5775, test loss: 0.8622, time: 33.29s\n",
      "Epoch 61 - train acc: 0.5990, train loss: 0.8554, test acc: 0.5765, test loss: 0.8616, time: 33.93s\n",
      "Epoch 62 - train acc: 0.5995, train loss: 0.8565, test acc: 0.5880, test loss: 0.8627, time: 34.37s\n",
      "Epoch 63 - train acc: 0.5961, train loss: 0.8552, test acc: 0.5840, test loss: 0.8600, time: 34.75s\n",
      "Epoch 64 - train acc: 0.6024, train loss: 0.8548, test acc: 0.5940, test loss: 0.8616, time: 35.15s\n",
      "Epoch 65 - train acc: 0.6052, train loss: 0.8461, test acc: 0.5825, test loss: 0.8515, time: 35.55s\n",
      "Epoch 66 - train acc: 0.6066, train loss: 0.8455, test acc: 0.5905, test loss: 0.8524, time: 36.04s\n",
      "Epoch 67 - train acc: 0.6024, train loss: 0.8512, test acc: 0.5975, test loss: 0.8587, time: 36.85s\n",
      "Epoch 68 - train acc: 0.6069, train loss: 0.8451, test acc: 0.5935, test loss: 0.8502, time: 37.67s\n",
      "Epoch 69 - train acc: 0.6120, train loss: 0.8376, test acc: 0.5925, test loss: 0.8440, time: 38.21s\n",
      "Epoch 70 - train acc: 0.6112, train loss: 0.8383, test acc: 0.6015, test loss: 0.8433, time: 38.67s\n",
      "Epoch 71 - train acc: 0.6082, train loss: 0.8418, test acc: 0.5930, test loss: 0.8477, time: 39.28s\n",
      "Epoch 72 - train acc: 0.6132, train loss: 0.8349, test acc: 0.5945, test loss: 0.8409, time: 39.75s\n",
      "Epoch 73 - train acc: 0.6128, train loss: 0.8356, test acc: 0.5990, test loss: 0.8408, time: 40.17s\n",
      "Epoch 74 - train acc: 0.6120, train loss: 0.8306, test acc: 0.6060, test loss: 0.8354, time: 40.69s\n",
      "Epoch 75 - train acc: 0.6136, train loss: 0.8296, test acc: 0.5955, test loss: 0.8356, time: 41.15s\n",
      "Epoch 76 - train acc: 0.6122, train loss: 0.8329, test acc: 0.5905, test loss: 0.8392, time: 41.59s\n",
      "Epoch 77 - train acc: 0.6152, train loss: 0.8273, test acc: 0.5990, test loss: 0.8332, time: 42.04s\n",
      "Epoch 78 - train acc: 0.6069, train loss: 0.8347, test acc: 0.6085, test loss: 0.8374, time: 42.46s\n",
      "Epoch 79 - train acc: 0.6130, train loss: 0.8273, test acc: 0.6030, test loss: 0.8319, time: 42.91s\n",
      "Epoch 80 - train acc: 0.6131, train loss: 0.8275, test acc: 0.6030, test loss: 0.8304, time: 43.43s\n",
      "Epoch 81 - train acc: 0.6149, train loss: 0.8244, test acc: 0.6030, test loss: 0.8280, time: 44.01s\n",
      "Epoch 82 - train acc: 0.6145, train loss: 0.8183, test acc: 0.6060, test loss: 0.8225, time: 44.53s\n",
      "Epoch 83 - train acc: 0.6115, train loss: 0.8196, test acc: 0.6000, test loss: 0.8231, time: 46.42s\n",
      "Epoch 84 - train acc: 0.6136, train loss: 0.8141, test acc: 0.6050, test loss: 0.8175, time: 47.98s\n",
      "Epoch 85 - train acc: 0.6129, train loss: 0.8149, test acc: 0.5980, test loss: 0.8197, time: 49.09s\n",
      "Epoch 86 - train acc: 0.6190, train loss: 0.8090, test acc: 0.6015, test loss: 0.8130, time: 49.62s\n",
      "Epoch 87 - train acc: 0.6136, train loss: 0.8081, test acc: 0.6035, test loss: 0.8124, time: 50.06s\n",
      "Epoch 88 - train acc: 0.6126, train loss: 0.8028, test acc: 0.6090, test loss: 0.8044, time: 50.49s\n",
      "Epoch 89 - train acc: 0.6145, train loss: 0.7993, test acc: 0.5965, test loss: 0.8025, time: 50.92s\n",
      "Epoch 90 - train acc: 0.6202, train loss: 0.7976, test acc: 0.6200, test loss: 0.7978, time: 51.27s\n",
      "Epoch 91 - train acc: 0.6216, train loss: 0.7907, test acc: 0.6120, test loss: 0.7931, time: 51.67s\n",
      "Epoch 92 - train acc: 0.6369, train loss: 0.7891, test acc: 0.6265, test loss: 0.7926, time: 51.99s\n",
      "Epoch 93 - train acc: 0.6091, train loss: 0.7828, test acc: 0.6080, test loss: 0.7820, time: 52.32s\n",
      "Epoch 94 - train acc: 0.6364, train loss: 0.7750, test acc: 0.6265, test loss: 0.7733, time: 52.57s\n",
      "Epoch 95 - train acc: 0.6156, train loss: 0.7706, test acc: 0.6075, test loss: 0.7685, time: 52.89s\n",
      "Epoch 96 - train acc: 0.6279, train loss: 0.7591, test acc: 0.6195, test loss: 0.7577, time: 53.43s\n",
      "Epoch 97 - train acc: 0.6464, train loss: 0.7507, test acc: 0.6425, test loss: 0.7492, time: 53.84s\n",
      "Epoch 98 - train acc: 0.6468, train loss: 0.7445, test acc: 0.6440, test loss: 0.7420, time: 54.22s\n",
      "Epoch 99 - train acc: 0.6542, train loss: 0.7318, test acc: 0.6520, test loss: 0.7294, time: 54.74s\n",
      "Epoch 100 - train acc: 0.6641, train loss: 0.7221, test acc: 0.6625, test loss: 0.7202, time: 55.31s\n",
      "Epoch 101 - train acc: 0.6640, train loss: 0.7123, test acc: 0.6685, test loss: 0.7095, time: 55.81s\n",
      "Epoch 102 - train acc: 0.6829, train loss: 0.7000, test acc: 0.6820, test loss: 0.6978, time: 56.24s\n",
      "Epoch 103 - train acc: 0.6916, train loss: 0.6898, test acc: 0.7005, test loss: 0.6888, time: 56.67s\n",
      "Epoch 104 - train acc: 0.7134, train loss: 0.6752, test acc: 0.7165, test loss: 0.6740, time: 57.20s\n",
      "Epoch 105 - train acc: 0.7285, train loss: 0.6582, test acc: 0.7300, test loss: 0.6564, time: 57.61s\n",
      "Epoch 106 - train acc: 0.7402, train loss: 0.6455, test acc: 0.7405, test loss: 0.6442, time: 58.05s\n",
      "Epoch 107 - train acc: 0.7220, train loss: 0.6446, test acc: 0.7295, test loss: 0.6434, time: 58.48s\n",
      "Epoch 108 - train acc: 0.7429, train loss: 0.6272, test acc: 0.7490, test loss: 0.6259, time: 58.91s\n",
      "Epoch 109 - train acc: 0.7469, train loss: 0.6148, test acc: 0.7560, test loss: 0.6115, time: 59.33s\n",
      "Epoch 110 - train acc: 0.7546, train loss: 0.6095, test acc: 0.7580, test loss: 0.6112, time: 59.78s\n",
      "Epoch 111 - train acc: 0.7620, train loss: 0.6022, test acc: 0.7590, test loss: 0.6020, time: 60.15s\n",
      "Epoch 112 - train acc: 0.7671, train loss: 0.5897, test acc: 0.7650, test loss: 0.5925, time: 60.60s\n",
      "Epoch 113 - train acc: 0.7690, train loss: 0.5802, test acc: 0.7645, test loss: 0.5838, time: 61.02s\n",
      "Epoch 114 - train acc: 0.7695, train loss: 0.5754, test acc: 0.7655, test loss: 0.5798, time: 61.59s\n",
      "Epoch 115 - train acc: 0.7775, train loss: 0.5675, test acc: 0.7660, test loss: 0.5721, time: 62.36s\n",
      "Epoch 116 - train acc: 0.7788, train loss: 0.5598, test acc: 0.7745, test loss: 0.5661, time: 63.34s\n",
      "Epoch 117 - train acc: 0.7851, train loss: 0.5593, test acc: 0.7800, test loss: 0.5659, time: 63.95s\n",
      "Epoch 118 - train acc: 0.7880, train loss: 0.5462, test acc: 0.7820, test loss: 0.5540, time: 64.41s\n",
      "Epoch 119 - train acc: 0.7921, train loss: 0.5431, test acc: 0.7775, test loss: 0.5512, time: 64.96s\n",
      "Epoch 120 - train acc: 0.7923, train loss: 0.5393, test acc: 0.7835, test loss: 0.5475, time: 65.46s\n",
      "Epoch 121 - train acc: 0.7931, train loss: 0.5346, test acc: 0.7845, test loss: 0.5421, time: 65.97s\n",
      "Epoch 122 - train acc: 0.7884, train loss: 0.5374, test acc: 0.7795, test loss: 0.5463, time: 66.42s\n",
      "Epoch 123 - train acc: 0.8016, train loss: 0.5237, test acc: 0.7940, test loss: 0.5313, time: 66.99s\n",
      "Epoch 124 - train acc: 0.8045, train loss: 0.5184, test acc: 0.7975, test loss: 0.5277, time: 67.51s\n",
      "Epoch 125 - train acc: 0.8053, train loss: 0.5112, test acc: 0.7980, test loss: 0.5199, time: 67.90s\n",
      "Epoch 126 - train acc: 0.8056, train loss: 0.5085, test acc: 0.7995, test loss: 0.5185, time: 68.36s\n",
      "Epoch 127 - train acc: 0.8055, train loss: 0.5060, test acc: 0.7980, test loss: 0.5162, time: 68.82s\n",
      "Epoch 128 - train acc: 0.8057, train loss: 0.5016, test acc: 0.8000, test loss: 0.5104, time: 69.75s\n",
      "Epoch 129 - train acc: 0.8069, train loss: 0.4996, test acc: 0.8015, test loss: 0.5075, time: 70.43s\n",
      "Epoch 130 - train acc: 0.8091, train loss: 0.5006, test acc: 0.8030, test loss: 0.5080, time: 71.02s\n",
      "Epoch 131 - train acc: 0.8101, train loss: 0.4966, test acc: 0.8010, test loss: 0.5046, time: 71.47s\n",
      "Epoch 132 - train acc: 0.8127, train loss: 0.4966, test acc: 0.8005, test loss: 0.5041, time: 71.91s\n",
      "Epoch 133 - train acc: 0.8145, train loss: 0.4908, test acc: 0.8040, test loss: 0.4995, time: 72.35s\n",
      "Epoch 134 - train acc: 0.8130, train loss: 0.4882, test acc: 0.8095, test loss: 0.4947, time: 72.81s\n",
      "Epoch 135 - train acc: 0.8119, train loss: 0.4947, test acc: 0.8030, test loss: 0.5000, time: 73.48s\n",
      "Epoch 136 - train acc: 0.8130, train loss: 0.4860, test acc: 0.8040, test loss: 0.4939, time: 73.93s\n",
      "Epoch 137 - train acc: 0.8101, train loss: 0.4935, test acc: 0.8040, test loss: 0.5014, time: 74.40s\n",
      "Epoch 138 - train acc: 0.8139, train loss: 0.4859, test acc: 0.8100, test loss: 0.4948, time: 75.04s\n",
      "Epoch 139 - train acc: 0.8136, train loss: 0.4857, test acc: 0.8065, test loss: 0.4945, time: 75.54s\n",
      "Epoch 140 - train acc: 0.8149, train loss: 0.4814, test acc: 0.8130, test loss: 0.4904, time: 76.01s\n",
      "Epoch 141 - train acc: 0.8156, train loss: 0.4780, test acc: 0.8130, test loss: 0.4858, time: 76.83s\n",
      "Epoch 142 - train acc: 0.8147, train loss: 0.4799, test acc: 0.8120, test loss: 0.4896, time: 77.41s\n",
      "Epoch 143 - train acc: 0.8184, train loss: 0.4751, test acc: 0.8120, test loss: 0.4843, time: 78.01s\n",
      "Epoch 144 - train acc: 0.8159, train loss: 0.4793, test acc: 0.8140, test loss: 0.4904, time: 78.49s\n",
      "Epoch 145 - train acc: 0.8201, train loss: 0.4759, test acc: 0.8090, test loss: 0.4828, time: 78.95s\n",
      "Epoch 146 - train acc: 0.8213, train loss: 0.4725, test acc: 0.8160, test loss: 0.4787, time: 79.66s\n",
      "Epoch 147 - train acc: 0.8185, train loss: 0.4713, test acc: 0.8120, test loss: 0.4794, time: 80.48s\n",
      "Epoch 148 - train acc: 0.8180, train loss: 0.4725, test acc: 0.8205, test loss: 0.4813, time: 81.34s\n",
      "Epoch 149 - train acc: 0.8184, train loss: 0.4685, test acc: 0.8160, test loss: 0.4779, time: 81.78s\n",
      "Epoch 150 - train acc: 0.8199, train loss: 0.4722, test acc: 0.8120, test loss: 0.4814, time: 82.21s\n"
     ]
    }
   ],
   "source": [
    "# train and test model\n",
    "train_model = train(model, x_train_scaled, y_train, x_test_scaled, y_test, epochs=150, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the codes above was modified from https://github.com/lionelmessi6410/Neural-Networks-from-Scratch/blob/main/model.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FANDIS_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
